{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Webinar Day 5- Rohan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvypt8HoLodk"
      },
      "source": [
        "# **Natural Language Processing Webinar - Day 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnPibI5nLygF"
      },
      "source": [
        "Natural Language Processing can be done using a number of libraries. Some are - \n",
        "\n",
        "*   NLTK\n",
        "*   Gensim\n",
        "*   SpaCy\n",
        " \n",
        " We are going to be using NLTK. It is one of the most basic ones and easy to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMTNh7XJMAHu"
      },
      "source": [
        "NLTK --> Natural Language Toolkit  (Free open source library)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxFHxbY338L8",
        "outputId": "b3e5a96f-7e3a-4006-e930-8e81fca630a2"
      },
      "source": [
        "pip install nltk                          # Install NLTK library"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPU2HwAL4jcU"
      },
      "source": [
        "import nltk                                # Import the NLTK library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhNnH-jqLHDO",
        "outputId": "b0e1e1a6-cd78-4f7c-c5e7-9c336d6dbf8e"
      },
      "source": [
        "nltk.download()                            # Access the NLTK modules from the library"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbrB8DCOLaLc",
        "outputId": "58939a14-e557-4430-a751-6d7b89e0057e"
      },
      "source": [
        "nltk.download('gutenberg')                     # Download the Gutenberg corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiaLO75IWSof"
      },
      "source": [
        "from nltk.corpus import gutenberg              # Importing the package so we don't have to call it again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8D5SRNmTjuW",
        "outputId": "9bebbfb9-4d5e-4bbb-fb3b-9e9b9ebef842"
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()                # Gives out a list of .txt files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFfFLsPkUJRu"
      },
      "source": [
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrVIPMpKU0F-",
        "outputId": "3d528807-050c-4c03-e9c3-2fff3643f1c2"
      },
      "source": [
        "emma"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctJB5NJ9VvOm"
      },
      "source": [
        "With the slicing method , we can look at the first 100 words in our text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CbKZze0U8-l",
        "outputId": "32694268-c173-4130-c821-9bc4dfa65c41"
      },
      "source": [
        "emma[0 : 100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Emma',\n",
              " 'by',\n",
              " 'Jane',\n",
              " 'Austen',\n",
              " '1816',\n",
              " ']',\n",
              " 'VOLUME',\n",
              " 'I',\n",
              " 'CHAPTER',\n",
              " 'I',\n",
              " 'Emma',\n",
              " 'Woodhouse',\n",
              " ',',\n",
              " 'handsome',\n",
              " ',',\n",
              " 'clever',\n",
              " ',',\n",
              " 'and',\n",
              " 'rich',\n",
              " ',',\n",
              " 'with',\n",
              " 'a',\n",
              " 'comfortable',\n",
              " 'home',\n",
              " 'and',\n",
              " 'happy',\n",
              " 'disposition',\n",
              " ',',\n",
              " 'seemed',\n",
              " 'to',\n",
              " 'unite',\n",
              " 'some',\n",
              " 'of',\n",
              " 'the',\n",
              " 'best',\n",
              " 'blessings',\n",
              " 'of',\n",
              " 'existence',\n",
              " ';',\n",
              " 'and',\n",
              " 'had',\n",
              " 'lived',\n",
              " 'nearly',\n",
              " 'twenty',\n",
              " '-',\n",
              " 'one',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'with',\n",
              " 'very',\n",
              " 'little',\n",
              " 'to',\n",
              " 'distress',\n",
              " 'or',\n",
              " 'vex',\n",
              " 'her',\n",
              " '.',\n",
              " 'She',\n",
              " 'was',\n",
              " 'the',\n",
              " 'youngest',\n",
              " 'of',\n",
              " 'the',\n",
              " 'two',\n",
              " 'daughters',\n",
              " 'of',\n",
              " 'a',\n",
              " 'most',\n",
              " 'affectionate',\n",
              " ',',\n",
              " 'indulgent',\n",
              " 'father',\n",
              " ';',\n",
              " 'and',\n",
              " 'had',\n",
              " ',',\n",
              " 'in',\n",
              " 'consequence',\n",
              " 'of',\n",
              " 'her',\n",
              " 'sister',\n",
              " \"'\",\n",
              " 's',\n",
              " 'marriage',\n",
              " ',',\n",
              " 'been',\n",
              " 'mistress',\n",
              " 'of',\n",
              " 'his',\n",
              " 'house',\n",
              " 'from',\n",
              " 'a',\n",
              " 'very',\n",
              " 'early',\n",
              " 'period',\n",
              " '.',\n",
              " 'Her']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opAwtp4ihvyG"
      },
      "source": [
        "We can also find out the number of sentences , the number of words & number of characters in any particular text we are looking at.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQVR4805W-rv"
      },
      "source": [
        "For this , we have to download the **Punkt Sentence Tokenizer**\n",
        "\n",
        "This tokenizer divides a text into a list of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGZlGi0HCjXa",
        "outputId": "c7b11479-3f15-4e63-a4d5-fe96a5bf57da"
      },
      "source": [
        "nltk.download('punkt')               "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aB7ocO8XRPF",
        "outputId": "ff110037-ce41-4890-fa79-fed15e5becae"
      },
      "source": [
        "characters = len(gutenberg.raw('austen-emma.txt')) \n",
        "words      = len(gutenberg.words('austen-emma.txt'))\n",
        "sents      = len(gutenberg.sents('austen-emma.txt'))\n",
        "\n",
        "print('No. of Characters: ', characters)\n",
        "print('No. of Words: ', words)\n",
        "print('No. of Sentences: ', sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of Characters:  887071\n",
            "No. of Words:  192427\n",
            "No. of Sentences:  7752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgusQaJQkUuk"
      },
      "source": [
        "# **Text Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBdmpMQLkXx9"
      },
      "source": [
        "Data pre-processing is the process of making the machine understand things better or making the input more machine understandable.\n",
        "Let us look at the methods we discussed in our PPT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Gdmh2ykdFz"
      },
      "source": [
        "## **1) Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMnzf6-ak3Ie"
      },
      "source": [
        "Tokenization is the process of breaking text up into smaller chunks as per our requirements. As we saw , to perform this we require the 'punkt' package from NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBfphmRdk9mQ"
      },
      "source": [
        "### **Word Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0q7SiqvlCaN"
      },
      "source": [
        "This method basically breaks down the sentence into words based on the spacing between each word.\n",
        "\n",
        "For performing tokenization , we require the \"tokenize\" module from NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cLKal2okOv2"
      },
      "source": [
        "from nltk.tokenize import word_tokenize                # Module for Word Tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_VHeO9YlgvV",
        "outputId": "939dcf17-e062-4c17-f512-0a1eba05d350"
      },
      "source": [
        "sentence = \"My name is Rohan Mathur and I like to study natural language processing\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "type(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvrrygkXlvbM",
        "outputId": "6668e25e-be14-4518-9595-c8cc71334236"
      },
      "source": [
        "print(words)                                            # Prints out the individual words in a sentence "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['My', 'name', 'is', 'Rohan', 'Mathur', 'and', 'I', 'like', 'to', 'study', 'natural', 'language', 'processing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIUMfJsmm0ZQ"
      },
      "source": [
        "We can also \"sentence\" tokenize a paragraph containing lots of sentences. Let us have a look at the implementation of this as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e5KWv53ntJj"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmE7l1z3nHFk",
        "outputId": "d8ff92d6-6994-4763-fb1c-c0fb1392089a"
      },
      "source": [
        "para  = \"The weather is good today. It is 27 Celsius. I am really thirsty. I want water\"\n",
        "\n",
        "sents = sent_tokenize(para)                       #Seperates the sentences from a paragraph\n",
        "\n",
        "print(sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The weather is good today.', 'It is 27 Celsius.', 'I am really thirsty.', 'I want water']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWjTU8molDK"
      },
      "source": [
        "### **Lowercasing**\n",
        "\n",
        "\n",
        "Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTTdh3WbqjxM",
        "outputId": "bdd770dd-eabe-4cb8-cbc9-c42a5a260c4a"
      },
      "source": [
        "for sentences in sents:\n",
        "    \n",
        "    print(sentences.lower())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the weather is good today.\n",
            "it is 27 celsius.\n",
            "i am really thirsty.\n",
            "i want water\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6nao5wEr0xZ"
      },
      "source": [
        "### **Punctuation Removal**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyAbTB0jr563"
      },
      "source": [
        "Punctuations are of little use in NLP and can interfere when we use our data for further modelling , so they are removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDfN34xHsCxg"
      },
      "source": [
        "We use a module called -  ```RegexpTokenizer``` from the ```tokenize``` module in NLTK for this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W06eRmkIrD8I",
        "outputId": "5be13820-2256-4268-bd66-e840e2b71b3b"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "result    = tokenizer.tokenize(\"The vaccine is out! When is college going to reopen??\")\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'vaccine', 'is', 'out', 'When', 'is', 'college', 'going', 'to', 'reopen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcc2S1-fsw31"
      },
      "source": [
        "### **Stop Words Removal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czdeMV1s5Lu"
      },
      "source": [
        "Stop words are words which occur frequently in a corpus. e.g a, an, the, in. Frequently occurring words are removed from the corpus for the sake of text-normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMDmfeoxuhnu",
        "outputId": "6e42fb84-623b-4866-b4f5-c75876ed11da"
      },
      "source": [
        "nltk.download('stopwords')                             # As punkt was to be downloaded for tokenizing , same has to be done with this"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVz5a9QIswli"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNi8BlUducTm"
      },
      "source": [
        "Let us have a look at what all stopwords are included "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "SgnBu4OWucEM",
        "outputId": "8c5b305f-1451-4d63-b0fd-7a11bcd7bbfc"
      },
      "source": [
        "\", \".join(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1BemjPowpDo"
      },
      "source": [
        "stop_words = stopwords.words('english')                     # A list containing all stop words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnmNZfoTvkie"
      },
      "source": [
        "para_2 = \"Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, in its pursuit to fill the gap between human communication and computer understanding.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHo77hYyvxY2",
        "outputId": "c607d38f-6b79-48d3-e50a-29ef23c5c7d0"
      },
      "source": [
        "tokenizer    = RegexpTokenizer(r'\\w+')                                     # Remove all punctuations first\n",
        "no_puncts    = tokenizer.tokenize(para_2)                                  # Iterate through each of the elements (words) in no_puncts & see if they exist in stop_words\n",
        "no_stopwords = [word for word in no_puncts if not word in stop_words]      # If yes , remove them\n",
        "\n",
        "no_stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'NLP',\n",
              " 'branch',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'helps',\n",
              " 'computers',\n",
              " 'understand',\n",
              " 'interpret',\n",
              " 'manipulate',\n",
              " 'human',\n",
              " 'language',\n",
              " 'NLP',\n",
              " 'draws',\n",
              " 'many',\n",
              " 'disciplines',\n",
              " 'including',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'computational',\n",
              " 'linguistics',\n",
              " 'pursuit',\n",
              " 'fill',\n",
              " 'gap',\n",
              " 'human',\n",
              " 'communication',\n",
              " 'computer',\n",
              " 'understanding']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax_9EvTFzI0R"
      },
      "source": [
        "### **Stemming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI6xpHJxzS3j"
      },
      "source": [
        "Often we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n",
        "\n",
        "The stemming and lemmatization process are hand-written regex rules written find the root word.\n",
        "\n",
        "\n",
        "The three major stemming algorithms in use nowadays:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Porter:** It is the most commonly used stemmer nowadays. It is one of the few stemmers that actually have Java support and it is also the most computationally intensive of the algorithms. It is also the oldest stemming algorithm by a large margin.\n",
        "\n",
        "2.   **Snowball:** This is an improvement over porter. It is slightly faster computation time than porter, with a reasonably large community around it.\n",
        "\n",
        "3.   **Lancaster:** It is a very aggressive stemming algorithm. With Porter and Snowball, the stemmed representations are intuitive to a reader, not so with Lancaster, as many shorter words will become totally confusing. (*This method was invented in the Lancaster University & has been named after that*)\n",
        "\n",
        "\n",
        "#### **All three can be done using ```nltk``` library**\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRhzqfEp4UuF"
      },
      "source": [
        "We will be using the Porter Stemmer for demonstration purposes for this webinar. You can try others as well & see how the results pan out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyC1NDeL40wA"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7_MXUxH4-ZY",
        "outputId": "c42bd08c-b95b-4c61-8ac1-69ed6e0a37ff"
      },
      "source": [
        "to_be_stemmed = ['Trouble', 'Troubling' , 'Eat' , 'Eating', \n",
        "                 'Stroked' , 'Stroke' , 'Fake','Faking']\n",
        "\n",
        "stemmed_words = [stemmer.stem(i) for i in to_be_stemmed]          # Iterate each word in the list\n",
        "\n",
        "print(stemmed_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['troubl', 'troubl', 'eat', 'eat', 'stroke', 'stroke', 'fake', 'fake']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9DWEpC4xV3"
      },
      "source": [
        "**PorterStemmer** is known for its simplicity and speed. It is commonly useful in Information Retrieval Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvyNOde657GQ"
      },
      "source": [
        "### **Lemmatization**\n",
        "\n",
        "It is another process of reducing inflection from words. The way its different from stemming is that it reduces words to their origins which have actual meaning. Stemming sometimes generates words which are not even words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqcXA6Es6yJB",
        "outputId": "17f0e60f-5fe2-4ae2-e2e3-571d5f65bb07"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqRZUfeD7BrB"
      },
      "source": [
        "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus. You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udLEK1Fk6l2o"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahGPzbPP4Sfz",
        "outputId": "685649ad-b62e-4e91-f775-12659228488c"
      },
      "source": [
        "to_be_lemmed     = ['rocks' , 'corpora', 'goods']\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(i) for i in to_be_lemmed] \n",
        "\n",
        "print(lemmatized_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rock', 'corpus', 'good']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exS2QAId98Nd"
      },
      "source": [
        "### **Counting Words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuYBjTrq-BFk"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Let's implement a simple function that is often used in Natural Language Processing: **Counting Word Frequencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGZUMvnc98Bp",
        "outputId": "c033a7c5-ea9a-4c0c-da14-e6275138cf25"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "passage = '''As I was waiting, a man came out of a side room, and at a glance I was sure he must be Long John. His\n",
        " left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed with wonderful dexterity\n",
        " , hopping about upon it like a bird. He was very tall and strong, with a face as big as a ham—plain and pale, but intelligent and smiling. \n",
        " Indeed, he seemed in the most cheerful spirits, whistling as he moved about among the tables, with a merry word or a slap on the shoulder for the \n",
        " more favoured of his guests.'''\n",
        "\n",
        "\n",
        "def count_words(text):\n",
        "    \n",
        "    counts = dict()                                 # Dictionary of { <word>: <count> } pairs to return\n",
        "    \n",
        "    text = text.lower()                             # Convert to lowercase\n",
        "    \n",
        "    tokenizer = RegexpTokenizer(r'\\w+')             # Remove Punctuations & Split text into tokens (words)\n",
        "    words = tokenizer.tokenize(text)\n",
        "    \n",
        "    \n",
        "    print(Counter(words))\n",
        "\n",
        "count_words(passage)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'a': 9, 'he': 6, 'the': 6, 'and': 5, 'as': 4, 'was': 4, 'with': 3, 'i': 2, 'of': 2, 'his': 2, 'left': 2, 'shoulder': 2, 'about': 2, 'waiting': 1, 'man': 1, 'came': 1, 'out': 1, 'side': 1, 'room': 1, 'at': 1, 'glance': 1, 'sure': 1, 'must': 1, 'be': 1, 'long': 1, 'john': 1, 'leg': 1, 'cut': 1, 'off': 1, 'close': 1, 'by': 1, 'hip': 1, 'under': 1, 'carried': 1, 'crutch': 1, 'which': 1, 'managed': 1, 'wonderful': 1, 'dexterity': 1, 'hopping': 1, 'upon': 1, 'it': 1, 'like': 1, 'bird': 1, 'very': 1, 'tall': 1, 'strong': 1, 'face': 1, 'big': 1, 'ham': 1, 'plain': 1, 'pale': 1, 'but': 1, 'intelligent': 1, 'smiling': 1, 'indeed': 1, 'seemed': 1, 'in': 1, 'most': 1, 'cheerful': 1, 'spirits': 1, 'whistling': 1, 'moved': 1, 'among': 1, 'tables': 1, 'merry': 1, 'word': 1, 'or': 1, 'slap': 1, 'on': 1, 'for': 1, 'more': 1, 'favoured': 1, 'guests': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE0HcisBjseB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}